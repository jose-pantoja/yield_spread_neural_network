{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y0KPpHD4C7s6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0fbb93-6dc9-494b-dfaa-4bea56cf8c42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1: Train Loss = 47.91761459842805, Val Loss = 20.579891204833984\n",
            "Epoch 2: Train Loss = 13.434885517243416, Val Loss = 9.319538116455078\n",
            "Epoch 3: Train Loss = 8.449710676746983, Val Loss = 3.788029670715332\n",
            "Epoch 4: Train Loss = 2.9695329896865355, Val Loss = 2.2581076622009277\n",
            "Epoch 5: Train Loss = 1.7725122321036555, Val Loss = 0.9515868425369263\n",
            "Epoch 6: Train Loss = 1.270678339465972, Val Loss = 1.2812327146530151\n",
            "Epoch 7: Train Loss = 0.7990342984276433, Val Loss = 0.18398714065551758\n",
            "Epoch 8: Train Loss = 0.3533350344627134, Val Loss = 0.5708064436912537\n",
            "Epoch 9: Train Loss = 0.4682333978914445, Val Loss = 0.21269479393959045\n",
            "Epoch 10: Train Loss = 0.1789192158368326, Val Loss = 0.1712891012430191\n",
            "Epoch 11: Train Loss = 0.2094864797207617, Val Loss = 0.07913187146186829\n",
            "Epoch 12: Train Loss = 0.13178763995247503, Val Loss = 0.10813816636800766\n",
            "Epoch 13: Train Loss = 0.0870274408690391, Val Loss = 0.08525873720645905\n",
            "Epoch 14: Train Loss = 0.08058914806573622, Val Loss = 0.05138614401221275\n",
            "Epoch 15: Train Loss = 0.054971819443087426, Val Loss = 0.034293562173843384\n",
            "Epoch 16: Train Loss = 0.047305205416294835, Val Loss = 0.025071734562516212\n",
            "Epoch 17: Train Loss = 0.043333047219822486, Val Loss = 0.03797148913145065\n",
            "Epoch 18: Train Loss = 0.04055536898874467, Val Loss = 0.03382086753845215\n",
            "Epoch 19: Train Loss = 0.026493872726155866, Val Loss = 0.018624048680067062\n",
            "Epoch 20: Train Loss = 0.02966275799178308, Val Loss = 0.01811378449201584\n",
            "Epoch 21: Train Loss = 0.03025516290818491, Val Loss = 0.02534974180161953\n",
            "Epoch 22: Train Loss = 0.025377924163495342, Val Loss = 0.02289322204887867\n",
            "Epoch 23: Train Loss = 0.023489569704378804, Val Loss = 0.02055836096405983\n",
            "Epoch 24: Train Loss = 0.02331452239905634, Val Loss = 0.02124733291566372\n",
            "Epoch 25: Train Loss = 0.023202430937559373, Val Loss = 0.018919887021183968\n",
            "Epoch 26: Train Loss = 0.02313823072660354, Val Loss = 0.02162782847881317\n",
            "Epoch 27: Train Loss = 0.02172627588433604, Val Loss = 0.021477673202753067\n",
            "Epoch 28: Train Loss = 0.020689799420295224, Val Loss = 0.019610736519098282\n",
            "Epoch 29: Train Loss = 0.021087780594825745, Val Loss = 0.020858706906437874\n",
            "Epoch 30: Train Loss = 0.020616357124620868, Val Loss = 0.020998580381274223\n",
            "Epoch 31: Train Loss = 0.020031267716999975, Val Loss = 0.02073858119547367\n",
            "Epoch 32: Train Loss = 0.019779761472055988, Val Loss = 0.020497720688581467\n",
            "Epoch 33: Train Loss = 0.01940741798570079, Val Loss = 0.020578516647219658\n",
            "Epoch 34: Train Loss = 0.019114075168486563, Val Loss = 0.021082991734147072\n",
            "Epoch 35: Train Loss = 0.01882117409859934, Val Loss = 0.02060084603726864\n",
            "Epoch 36: Train Loss = 0.018560250439951496, Val Loss = 0.02062905766069889\n",
            "Epoch 37: Train Loss = 0.01828430857389204, Val Loss = 0.020722350105643272\n",
            "Epoch 38: Train Loss = 0.01803713976856201, Val Loss = 0.020628968253731728\n",
            "Epoch 39: Train Loss = 0.017825223986179597, Val Loss = 0.02076467126607895\n",
            "Epoch 40: Train Loss = 0.0175762293079207, Val Loss = 0.020596059039235115\n",
            "Epoch 41: Train Loss = 0.017385969659493815, Val Loss = 0.02072785049676895\n",
            "Epoch 42: Train Loss = 0.017171477057760762, Val Loss = 0.020740900188684464\n",
            "Epoch 43: Train Loss = 0.016968557611107826, Val Loss = 0.020673932507634163\n",
            "Epoch 44: Train Loss = 0.0167858335039308, Val Loss = 0.020707041025161743\n",
            "Epoch 45: Train Loss = 0.01659976366546846, Val Loss = 0.02065409906208515\n",
            "Epoch 46: Train Loss = 0.01644371822476387, Val Loss = 0.02066568098962307\n",
            "Epoch 47: Train Loss = 0.016276203576595552, Val Loss = 0.02061370015144348\n",
            "Epoch 48: Train Loss = 0.016119931313780048, Val Loss = 0.02060154639184475\n",
            "Epoch 49: Train Loss = 0.015965748337968703, Val Loss = 0.020576991140842438\n",
            "Epoch 50: Train Loss = 0.015821945222635424, Val Loss = 0.02053484320640564\n",
            "Epoch 51: Train Loss = 0.015689217034847505, Val Loss = 0.020474763587117195\n",
            "Epoch 52: Train Loss = 0.015554768243624319, Val Loss = 0.020441532135009766\n",
            "Epoch 53: Train Loss = 0.015425680926249873, Val Loss = 0.020408177748322487\n",
            "Epoch 54: Train Loss = 0.01529676491214383, Val Loss = 0.02035181038081646\n",
            "Epoch 55: Train Loss = 0.015179216140700925, Val Loss = 0.020320473238825798\n",
            "Epoch 56: Train Loss = 0.01506327014536627, Val Loss = 0.020285246893763542\n",
            "Epoch 57: Train Loss = 0.014946973852572902, Val Loss = 0.02025485783815384\n",
            "Epoch 58: Train Loss = 0.014832916819760877, Val Loss = 0.020211193710565567\n",
            "Epoch 59: Train Loss = 0.014725212188016984, Val Loss = 0.020173329859972\n",
            "Epoch 60: Train Loss = 0.014619723262806092, Val Loss = 0.020125580951571465\n",
            "Epoch 61: Train Loss = 0.014516903988776668, Val Loss = 0.02007342502474785\n",
            "Epoch 62: Train Loss = 0.01441776289814903, Val Loss = 0.020022844895720482\n",
            "Epoch 63: Train Loss = 0.014320997040598624, Val Loss = 0.019972270354628563\n",
            "Epoch 64: Train Loss = 0.014227122068405151, Val Loss = 0.01992291770875454\n",
            "Epoch 65: Train Loss = 0.014136005132909744, Val Loss = 0.019871722906827927\n",
            "Epoch 66: Train Loss = 0.014047383420890378, Val Loss = 0.01982135884463787\n",
            "Epoch 67: Train Loss = 0.01396088338186664, Val Loss = 0.019766315817832947\n",
            "Epoch 68: Train Loss = 0.013877002942946649, Val Loss = 0.019713658839464188\n",
            "Epoch 69: Train Loss = 0.01379547268152237, Val Loss = 0.019669445231556892\n",
            "Epoch 70: Train Loss = 0.013715268022591067, Val Loss = 0.01962026208639145\n",
            "Epoch 71: Train Loss = 0.013635775975642665, Val Loss = 0.01955801621079445\n",
            "Epoch 72: Train Loss = 0.013560843623934253, Val Loss = 0.019488463178277016\n",
            "Epoch 73: Train Loss = 0.01348927869431434, Val Loss = 0.019435590133070946\n",
            "Epoch 74: Train Loss = 0.013419053907836638, Val Loss = 0.01937701366841793\n",
            "Epoch 75: Train Loss = 0.01334834327140162, Val Loss = 0.019313132390379906\n",
            "Epoch 76: Train Loss = 0.013280989962720102, Val Loss = 0.019247153773903847\n",
            "Epoch 77: Train Loss = 0.013215767399918648, Val Loss = 0.019191792234778404\n",
            "Epoch 78: Train Loss = 0.01315137725924292, Val Loss = 0.019135883077979088\n",
            "Epoch 79: Train Loss = 0.013087644572219542, Val Loss = 0.01908276602625847\n",
            "Epoch 80: Train Loss = 0.013024733672218939, Val Loss = 0.01902920939028263\n",
            "Epoch 81: Train Loss = 0.012963005852314734, Val Loss = 0.01897687464952469\n",
            "Epoch 82: Train Loss = 0.01290212090938322, Val Loss = 0.018926844000816345\n",
            "Epoch 83: Train Loss = 0.012842443621447009, Val Loss = 0.01887168362736702\n",
            "Epoch 84: Train Loss = 0.012784033653236205, Val Loss = 0.018818611279129982\n",
            "Epoch 85: Train Loss = 0.012727147989696073, Val Loss = 0.018763234838843346\n",
            "Epoch 86: Train Loss = 0.012671293029862066, Val Loss = 0.018708793446421623\n",
            "Epoch 87: Train Loss = 0.01261659435206844, Val Loss = 0.018650265410542488\n",
            "Epoch 88: Train Loss = 0.012562987364588244, Val Loss = 0.01859516277909279\n",
            "Epoch 89: Train Loss = 0.012509444007469762, Val Loss = 0.01853754185140133\n",
            "Epoch 90: Train Loss = 0.012456992520920692, Val Loss = 0.018481306731700897\n",
            "Epoch 91: Train Loss = 0.012405210925686744, Val Loss = 0.01842375285923481\n",
            "Epoch 92: Train Loss = 0.012354077350708747, Val Loss = 0.018367933109402657\n",
            "Epoch 93: Train Loss = 0.012303126014528735, Val Loss = 0.018309473991394043\n",
            "Epoch 94: Train Loss = 0.01225293097236464, Val Loss = 0.0182530228048563\n",
            "Epoch 95: Train Loss = 0.01220294454645726, Val Loss = 0.018195882439613342\n",
            "Epoch 96: Train Loss = 0.012153752148151398, Val Loss = 0.01814248599112034\n",
            "Epoch 97: Train Loss = 0.012104383579665614, Val Loss = 0.018087508156895638\n",
            "Epoch 98: Train Loss = 0.012055656121623131, Val Loss = 0.01803518831729889\n",
            "Epoch 99: Train Loss = 0.012007133013779116, Val Loss = 0.017982706427574158\n",
            "Epoch 100: Train Loss = 0.011958842135725482, Val Loss = 0.01792461797595024\n",
            "Test Loss: 0.06784537205329308\n",
            "Instrument Rankings:\n",
            "Rank 1: Instrument yield_spread_LRC30APR_Index_GT5_Govt (2.7322120666503906)\n",
            "Rank 2: Instrument yield_spread_LRC30APR_Index_USGG5YR_Index (2.5653114318847656)\n",
            "Rank 3: Instrument yield_spread_LRC30APR_Index_MTGEFNCL_Index (1.558294653892517)\n",
            "Rank 4: Instrument yield_spread_MTGEFNCL_Index_GT5_Govt (1.4765329360961914)\n",
            "Rank 5: Instrument yield_spread_MTGEFNCL_Index_USGG5YR_Index (1.156231164932251)\n",
            "Rank 6: Instrument yield_spread_USGG5YR_Index_GT5_Govt (0.03396240994334221)\n",
            "Rank 7: Instrument yield_spread_GT5_Govt_USGG5YR_Index (0.0222932081669569)\n",
            "Rank 8: Instrument yield_spread_USGG5YR_Index_MTGEFNCL_Index (-1.2064456939697266)\n",
            "Rank 9: Instrument yield_spread_GT5_Govt_MTGEFNCL_Index (-1.3464757204055786)\n",
            "Rank 10: Instrument yield_spread_MTGEFNCL_Index_LRC30APR_Index (-1.5362117290496826)\n",
            "Rank 11: Instrument yield_spread_GT5_Govt_LRC30APR_Index (-2.618637800216675)\n",
            "Rank 12: Instrument yield_spread_USGG5YR_Index_LRC30APR_Index (-2.660696506500244)\n"
          ]
        }
      ],
      "source": [
        "# initialization code\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# path to drive where csv file is read from\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/MyDrive/BBGdatasets/sampleDailyInputData_12-27-21.csv\"\n",
        "\n",
        "# load historical data from path into a pandas DataFrame\n",
        "df = pd.read_csv(datadir, index_col=0, header=[0,1], parse_dates=True)\n",
        "df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "\n",
        "# transform existing DataFrame by calculating yield spreads and assigning to new DataFrame\n",
        "def yield_spreads_calc(df):\n",
        "    return (df\n",
        "     .assign(yield_spread_MTGEFNCL_Index_LRC30APR_Index = df['MTGEFNCL Index PX_LAST'] - df['LRC30APR Index PX_LAST'],\n",
        "             yield_spread_MTGEFNCL_Index_GT5_Govt = df['MTGEFNCL Index PX_LAST'] - df['GT5 Govt PX_LAST'],\n",
        "             yield_spread_MTGEFNCL_Index_USGG5YR_Index = df['MTGEFNCL Index PX_LAST'] - df['USGG5YR Index PX_LAST'],\n",
        "             yield_spread_LRC30APR_Index_MTGEFNCL_Index = df['LRC30APR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST'],\n",
        "             yield_spread_LRC30APR_Index_GT5_Govt = df['LRC30APR Index PX_LAST'] - df['GT5 Govt PX_LAST'],\n",
        "             yield_spread_LRC30APR_Index_USGG5YR_Index = df['LRC30APR Index PX_LAST'] - df['USGG5YR Index PX_LAST'],\n",
        "             yield_spread_GT5_Govt_MTGEFNCL_Index = df['GT5 Govt PX_LAST'] - df['MTGEFNCL Index PX_LAST'],\n",
        "             yield_spread_GT5_Govt_LRC30APR_Index = df['GT5 Govt PX_LAST'] - df['LRC30APR Index PX_LAST'],\n",
        "             yield_spread_GT5_Govt_USGG5YR_Index = df['GT5 Govt PX_LAST'] - df['USGG5YR Index PX_LAST'],\n",
        "             yield_spread_USGG5YR_Index_MTGEFNCL_Index = df['USGG5YR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST'],\n",
        "             yield_spread_USGG5YR_Index_LRC30APR_Index = df['USGG5YR Index PX_LAST'] - df['LRC30APR Index PX_LAST'],\n",
        "             yield_spread_USGG5YR_Index_GT5_Govt = df['USGG5YR Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "            )\n",
        "    )\n",
        "yield_spreads_df = yield_spreads_calc(df)\n",
        "\n",
        "# number of yield spreads, adjust accordingly\n",
        "n = 12\n",
        "\n",
        "# seperate yield spreads from other market information\n",
        "yield_spreads = yield_spreads_df.iloc[:, -n:]\n",
        "other_columns = yield_spreads_df.iloc[:, :-n]\n",
        "\n",
        "# concatenate yield spreads with other market information to restructure into new DataFrame\n",
        "df_new = pd.concat([yield_spreads, other_columns], axis=1)\n",
        "\n",
        "# # assign the new DataFrame to df_imp to not overwrite original DataFrame as well as do MICE Imputation\n",
        "imputer = IterativeImputer(random_state=100, max_iter=10)\n",
        "imputer.fit(df_new)\n",
        "df_imp = imputer.transform(df_new)\n",
        "data = pd.DataFrame(df_imp)\n",
        "\n",
        "# separate the yield spreads which are our targets from other market information again and into NumPy arrays\n",
        "yield_spreads = data.iloc[:, :n].values\n",
        "market_info = data.iloc[:, n:].values\n",
        "\n",
        "# device configuration for using Nvidia GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# convert the data to PyTorch tensors of type torch.float32 and run on GPU if available\n",
        "X_yield_spreads = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "X_market_info = torch.tensor(market_info, dtype=torch.float32).to(device)\n",
        "\n",
        "# concatenate the yield spreads and other market information tensors as well as run on GPU if available\n",
        "X = torch.cat((X_yield_spreads, X_market_info), dim=1).to(device)\n",
        "y = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "\n",
        "# split the data into training, validation, and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n",
        "\n",
        "# note that no scalers are needed as features are similar\n",
        "\n",
        "# define the neural network model named Spread\n",
        "class Spread(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output):\n",
        "        super(Spread, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.fc3 = nn.Linear(hidden_size_2, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# hyperparameters to control learning proccess of model\n",
        "input_size = data.shape[1]\n",
        "hidden_size_1 = 264\n",
        "hidden_size_2 = 132\n",
        "output = n\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# instantiating the neural network model with GPU if available\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "\n",
        "# define the Mean Squred Error loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# create dataloaders for training, validation, and testing sets calling whole class\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# training loop of model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    # iterate over training set batches\n",
        "    for inputs, targets in train_loader:\n",
        "        # convert inputs and targets to run on GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # update learnable weights of model as well as output input of inputs\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # MSE loss between inputs outputs and actual targets values\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # calculate average training loss for every epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # evaluate model on validation set without calculating gradients\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            val_outputs = model(inputs)\n",
        "            batch_loss = criterion(val_outputs, targets)\n",
        "\n",
        "            val_loss += batch_loss.item() * inputs.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    # display current Train Loss and Val Loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss}, Val Loss = {val_loss}\")\n",
        "\n",
        "# save only state_dict of model and where\n",
        "save_model_name = 'spread.pt'\n",
        "PATH = f\"/content/drive/MyDrive/BBGdatasets/MLmodels/{save_model_name}\"\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# load loaded dictionary of model\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# evaluate model on test set similar to validation set\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        test_outputs = model(inputs)\n",
        "        batch_loss = criterion(test_outputs, targets)\n",
        "\n",
        "        test_loss += batch_loss.item() * inputs.size(0)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "\n",
        "# convert the predicted tensor back to a NumPy array\n",
        "predictions = test_outputs.cpu().numpy()\n",
        "\n",
        "# calculate the average predicted yield spread for each instrument\n",
        "avg_predicted_spreads = np.mean(predictions, axis=0)\n",
        "\n",
        "# get instrument names\n",
        "instrument_names = df_new.columns[:n]\n",
        "\n",
        "# create a list of tuples with instrument names and their average predicted spreads\n",
        "instrument_spreads = list(zip(instrument_names, avg_predicted_spreads))\n",
        "\n",
        "# rank all of the instruments based on their average predicted yield spreads\n",
        "ranked_instruments = sorted(instrument_spreads, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# print each instrument's ranking\n",
        "print(\"Instrument Rankings:\")\n",
        "for rank, (instrument_name, spread) in enumerate(ranked_instruments):\n",
        "    print(f\"Rank {rank+1}: Instrument {instrument_name} ({spread})\")"
      ]
    }
  ]
}