{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y0KPpHD4C7s6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc90adc-d7ed-4f0c-bf57-459d3e716574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1: Train Loss = 29.332378387451172, Val Loss = 386.52044677734375\n",
            "Epoch 2: Train Loss = 415.7914123535156, Val Loss = 35.54665756225586\n",
            "Epoch 3: Train Loss = 38.68523025512695, Val Loss = 1.9076331853866577\n",
            "Epoch 4: Train Loss = 1.9134094715118408, Val Loss = 2.1404941082000732\n",
            "Epoch 5: Train Loss = 2.0559113025665283, Val Loss = 1.9307889938354492\n",
            "Epoch 6: Train Loss = 1.8539625406265259, Val Loss = 1.5597529411315918\n",
            "Epoch 7: Train Loss = 1.4772883653640747, Val Loss = 1.0084574222564697\n",
            "Epoch 8: Train Loss = 0.9662413001060486, Val Loss = 0.6316562294960022\n",
            "Epoch 9: Train Loss = 0.6276220679283142, Val Loss = 0.3882787227630615\n",
            "Epoch 10: Train Loss = 0.4469040036201477, Val Loss = 0.23779164254665375\n",
            "Epoch 11: Train Loss = 0.29482343792915344, Val Loss = 0.4533054232597351\n",
            "Epoch 12: Train Loss = 0.46479472517967224, Val Loss = 0.16376321017742157\n",
            "Epoch 13: Train Loss = 0.15456026792526245, Val Loss = 0.31294944882392883\n",
            "Epoch 14: Train Loss = 0.2841548025608063, Val Loss = 0.16772189736366272\n",
            "Epoch 15: Train Loss = 0.14519916474819183, Val Loss = 0.3138452470302582\n",
            "Epoch 16: Train Loss = 0.3626094460487366, Val Loss = 0.10297400504350662\n",
            "Epoch 17: Train Loss = 0.10277149081230164, Val Loss = 0.12663857638835907\n",
            "Epoch 18: Train Loss = 0.11869201064109802, Val Loss = 0.16463319957256317\n",
            "Epoch 19: Train Loss = 0.16852213442325592, Val Loss = 0.09943837672472\n",
            "Epoch 20: Train Loss = 0.08844664692878723, Val Loss = 0.09409743547439575\n",
            "Epoch 21: Train Loss = 0.0896291509270668, Val Loss = 0.06277336180210114\n",
            "Epoch 22: Train Loss = 0.07830196619033813, Val Loss = 0.047782815992832184\n",
            "Epoch 23: Train Loss = 0.07808960974216461, Val Loss = 0.05089058727025986\n",
            "Epoch 24: Train Loss = 0.07063762843608856, Val Loss = 0.052437134087085724\n",
            "Epoch 25: Train Loss = 0.05324332416057587, Val Loss = 0.04444163665175438\n",
            "Epoch 26: Train Loss = 0.038030438125133514, Val Loss = 0.03841528296470642\n",
            "Epoch 27: Train Loss = 0.034577876329422, Val Loss = 0.04119522124528885\n",
            "Epoch 28: Train Loss = 0.04054330289363861, Val Loss = 0.04394514486193657\n",
            "Epoch 29: Train Loss = 0.040752653032541275, Val Loss = 0.042978931218385696\n",
            "Epoch 30: Train Loss = 0.03725547716021538, Val Loss = 0.033124037086963654\n",
            "Epoch 31: Train Loss = 0.031103435903787613, Val Loss = 0.02675706520676613\n",
            "Epoch 32: Train Loss = 0.030074704438447952, Val Loss = 0.02822771482169628\n",
            "Epoch 33: Train Loss = 0.02950638346374035, Val Loss = 0.03232904523611069\n",
            "Epoch 34: Train Loss = 0.026177914813160896, Val Loss = 0.03257957845926285\n",
            "Epoch 35: Train Loss = 0.0253482423722744, Val Loss = 0.0266761165112257\n",
            "Epoch 36: Train Loss = 0.027493605390191078, Val Loss = 0.023917419835925102\n",
            "Epoch 37: Train Loss = 0.03042963705956936, Val Loss = 0.023207593709230423\n",
            "Epoch 38: Train Loss = 0.025497736409306526, Val Loss = 0.023304559290409088\n",
            "Epoch 39: Train Loss = 0.019917942583560944, Val Loss = 0.021127188578248024\n",
            "Epoch 40: Train Loss = 0.018167855218052864, Val Loss = 0.023382337763905525\n",
            "Epoch 41: Train Loss = 0.021419409662485123, Val Loss = 0.027267923578619957\n",
            "Epoch 42: Train Loss = 0.02125362493097782, Val Loss = 0.027453739196062088\n",
            "Epoch 43: Train Loss = 0.01936080865561962, Val Loss = 0.020564528182148933\n",
            "Epoch 44: Train Loss = 0.01767386682331562, Val Loss = 0.017083723098039627\n",
            "Epoch 45: Train Loss = 0.01861106976866722, Val Loss = 0.019548123702406883\n",
            "Epoch 46: Train Loss = 0.01789041981101036, Val Loss = 0.021719109266996384\n",
            "Epoch 47: Train Loss = 0.016963951289653778, Val Loss = 0.018266508355736732\n",
            "Epoch 48: Train Loss = 0.0164351686835289, Val Loss = 0.017146313562989235\n",
            "Epoch 49: Train Loss = 0.01761123351752758, Val Loss = 0.019497143104672432\n",
            "Epoch 50: Train Loss = 0.018077678978443146, Val Loss = 0.018497683107852936\n",
            "Epoch 51: Train Loss = 0.017624100670218468, Val Loss = 0.014023457653820515\n",
            "Epoch 52: Train Loss = 0.0162945669144392, Val Loss = 0.015139058232307434\n",
            "Epoch 53: Train Loss = 0.015528540126979351, Val Loss = 0.020543785765767097\n",
            "Epoch 54: Train Loss = 0.01604491099715233, Val Loss = 0.020769067108631134\n",
            "Epoch 55: Train Loss = 0.01635053940117359, Val Loss = 0.018046898767352104\n",
            "Epoch 56: Train Loss = 0.01598266325891018, Val Loss = 0.017977867275476456\n",
            "Epoch 57: Train Loss = 0.015161857940256596, Val Loss = 0.01762969419360161\n",
            "Epoch 58: Train Loss = 0.01516750082373619, Val Loss = 0.014583233743906021\n",
            "Epoch 59: Train Loss = 0.01505959965288639, Val Loss = 0.01474512554705143\n",
            "Epoch 60: Train Loss = 0.014698543585836887, Val Loss = 0.017365457490086555\n",
            "Epoch 61: Train Loss = 0.014599179849028587, Val Loss = 0.01638125628232956\n",
            "Epoch 62: Train Loss = 0.014705528505146503, Val Loss = 0.01482235174626112\n",
            "Epoch 63: Train Loss = 0.014911464415490627, Val Loss = 0.015742601826786995\n",
            "Epoch 64: Train Loss = 0.014720188453793526, Val Loss = 0.015256499871611595\n",
            "Epoch 65: Train Loss = 0.01442208606749773, Val Loss = 0.014036563225090504\n",
            "Epoch 66: Train Loss = 0.014296657405793667, Val Loss = 0.01611059345304966\n",
            "Epoch 67: Train Loss = 0.014393252320587635, Val Loss = 0.017648642882704735\n",
            "Epoch 68: Train Loss = 0.01457731332629919, Val Loss = 0.015754004940390587\n",
            "Epoch 69: Train Loss = 0.01439395546913147, Val Loss = 0.015116789378225803\n",
            "Epoch 70: Train Loss = 0.014154618605971336, Val Loss = 0.015592878684401512\n",
            "Epoch 71: Train Loss = 0.014147999696433544, Val Loss = 0.014250453561544418\n",
            "Epoch 72: Train Loss = 0.014099208638072014, Val Loss = 0.014279049821197987\n",
            "Epoch 73: Train Loss = 0.014021104201674461, Val Loss = 0.015623651444911957\n",
            "Epoch 74: Train Loss = 0.013982358388602734, Val Loss = 0.014641822315752506\n",
            "Epoch 75: Train Loss = 0.013950861059129238, Val Loss = 0.013824176043272018\n",
            "Epoch 76: Train Loss = 0.01395593024790287, Val Loss = 0.01456308364868164\n",
            "Epoch 77: Train Loss = 0.013903177343308926, Val Loss = 0.014096495695412159\n",
            "Epoch 78: Train Loss = 0.013791540637612343, Val Loss = 0.01415226235985756\n",
            "Epoch 79: Train Loss = 0.01377870049327612, Val Loss = 0.015582871623337269\n",
            "Epoch 80: Train Loss = 0.013831553980708122, Val Loss = 0.015143517404794693\n",
            "Epoch 81: Train Loss = 0.013810894452035427, Val Loss = 0.014237461611628532\n",
            "Epoch 82: Train Loss = 0.013733710162341595, Val Loss = 0.014547456055879593\n",
            "Epoch 83: Train Loss = 0.013676525093615055, Val Loss = 0.013959692791104317\n",
            "Epoch 84: Train Loss = 0.013651642948389053, Val Loss = 0.013748415745794773\n",
            "Epoch 85: Train Loss = 0.013639137148857117, Val Loss = 0.014584735035896301\n",
            "Epoch 86: Train Loss = 0.013609504327178001, Val Loss = 0.014081022702157497\n",
            "Epoch 87: Train Loss = 0.013566388748586178, Val Loss = 0.013694914057850838\n",
            "Epoch 88: Train Loss = 0.013546443544328213, Val Loss = 0.01413350086659193\n",
            "Epoch 89: Train Loss = 0.01352777797728777, Val Loss = 0.013713899068534374\n",
            "Epoch 90: Train Loss = 0.013480707071721554, Val Loss = 0.01396311353892088\n",
            "Epoch 91: Train Loss = 0.013452135026454926, Val Loss = 0.014546235091984272\n",
            "Epoch 92: Train Loss = 0.013455217704176903, Val Loss = 0.013946966268122196\n",
            "Epoch 93: Train Loss = 0.013436063192784786, Val Loss = 0.01387878879904747\n",
            "Epoch 94: Train Loss = 0.013394246809184551, Val Loss = 0.013870307244360447\n",
            "Epoch 95: Train Loss = 0.013363449834287167, Val Loss = 0.013421586714684963\n",
            "Epoch 96: Train Loss = 0.013344530016183853, Val Loss = 0.013857911340892315\n",
            "Epoch 97: Train Loss = 0.013326896354556084, Val Loss = 0.013801059685647488\n",
            "Epoch 98: Train Loss = 0.013302994892001152, Val Loss = 0.013403896242380142\n",
            "Epoch 99: Train Loss = 0.013275490142405033, Val Loss = 0.013616912066936493\n",
            "Epoch 100: Train Loss = 0.013254055753350258, Val Loss = 0.013315782882273197\n",
            "Epoch 101: Train Loss = 0.013231380842626095, Val Loss = 0.013429936952888966\n",
            "Epoch 102: Train Loss = 0.013204951770603657, Val Loss = 0.013778459280729294\n",
            "Epoch 103: Train Loss = 0.013185449875891209, Val Loss = 0.013457383960485458\n",
            "Epoch 104: Train Loss = 0.01316863764077425, Val Loss = 0.013541915453970432\n",
            "Epoch 105: Train Loss = 0.013145589269697666, Val Loss = 0.01333529595285654\n",
            "Epoch 106: Train Loss = 0.013120289891958237, Val Loss = 0.01311015710234642\n",
            "Epoch 107: Train Loss = 0.013097924180328846, Val Loss = 0.013379906304180622\n",
            "Epoch 108: Train Loss = 0.013080783188343048, Val Loss = 0.013143619522452354\n",
            "Epoch 109: Train Loss = 0.013062708079814911, Val Loss = 0.013183154165744781\n",
            "Epoch 110: Train Loss = 0.013039511628448963, Val Loss = 0.013138726353645325\n",
            "Epoch 111: Train Loss = 0.013018058612942696, Val Loss = 0.012949667870998383\n",
            "Epoch 112: Train Loss = 0.012999773025512695, Val Loss = 0.013219442218542099\n",
            "Epoch 113: Train Loss = 0.012979792430996895, Val Loss = 0.01308536995202303\n",
            "Epoch 114: Train Loss = 0.012959133833646774, Val Loss = 0.013144675642251968\n",
            "Epoch 115: Train Loss = 0.012939299456775188, Val Loss = 0.013073562644422054\n",
            "Epoch 116: Train Loss = 0.012919915840029716, Val Loss = 0.012866934761404991\n",
            "Epoch 117: Train Loss = 0.012899767607450485, Val Loss = 0.013015922158956528\n",
            "Epoch 118: Train Loss = 0.012878761626780033, Val Loss = 0.01286389771848917\n",
            "Epoch 119: Train Loss = 0.012859464623034, Val Loss = 0.012977388687431812\n",
            "Epoch 120: Train Loss = 0.012840792536735535, Val Loss = 0.012862305156886578\n",
            "Epoch 121: Train Loss = 0.012820319272577763, Val Loss = 0.012775123119354248\n",
            "Epoch 122: Train Loss = 0.012799946591258049, Val Loss = 0.01284503098577261\n",
            "Epoch 123: Train Loss = 0.012780781835317612, Val Loss = 0.012742687948048115\n",
            "Epoch 124: Train Loss = 0.012761682271957397, Val Loss = 0.012899073772132397\n",
            "Epoch 125: Train Loss = 0.01274239644408226, Val Loss = 0.012727086432278156\n",
            "Epoch 126: Train Loss = 0.012722807936370373, Val Loss = 0.012773435562849045\n",
            "Epoch 127: Train Loss = 0.012703600339591503, Val Loss = 0.012653286568820477\n",
            "Epoch 128: Train Loss = 0.01268412359058857, Val Loss = 0.01266865711659193\n",
            "Epoch 129: Train Loss = 0.012664351612329483, Val Loss = 0.012659301981329918\n",
            "Epoch 130: Train Loss = 0.012645291164517403, Val Loss = 0.012577543035149574\n",
            "Epoch 131: Train Loss = 0.012626443058252335, Val Loss = 0.0125983115285635\n",
            "Epoch 132: Train Loss = 0.012607110664248466, Val Loss = 0.012474901974201202\n",
            "Epoch 133: Train Loss = 0.012587720528244972, Val Loss = 0.012597109191119671\n",
            "Epoch 134: Train Loss = 0.012568510137498379, Val Loss = 0.012468215078115463\n",
            "Epoch 135: Train Loss = 0.012549454346299171, Val Loss = 0.012602542527019978\n",
            "Epoch 136: Train Loss = 0.012530091218650341, Val Loss = 0.012375141493976116\n",
            "Epoch 137: Train Loss = 0.012510780245065689, Val Loss = 0.01254578959196806\n",
            "Epoch 138: Train Loss = 0.012491732835769653, Val Loss = 0.012273493222892284\n",
            "Epoch 139: Train Loss = 0.012472708709537983, Val Loss = 0.012581332586705685\n",
            "Epoch 140: Train Loss = 0.012453988194465637, Val Loss = 0.012156972661614418\n",
            "Epoch 141: Train Loss = 0.01243608258664608, Val Loss = 0.01265532337129116\n",
            "Epoch 142: Train Loss = 0.012419747188687325, Val Loss = 0.011912547051906586\n",
            "Epoch 143: Train Loss = 0.01240669284015894, Val Loss = 0.012914185412228107\n",
            "Epoch 144: Train Loss = 0.01240084134042263, Val Loss = 0.011513348668813705\n",
            "Epoch 145: Train Loss = 0.012411536648869514, Val Loss = 0.013629413209855556\n",
            "Epoch 146: Train Loss = 0.012460339814424515, Val Loss = 0.010672364383935928\n",
            "Epoch 147: Train Loss = 0.012599308043718338, Val Loss = 0.015650805085897446\n",
            "Epoch 148: Train Loss = 0.012960157357156277, Val Loss = 0.009371737018227577\n",
            "Epoch 149: Train Loss = 0.013866466470062733, Val Loss = 0.0226048044860363\n",
            "Epoch 150: Train Loss = 0.01613730564713478, Val Loss = 0.010503984987735748\n",
            "Epoch 151: Train Loss = 0.021510539576411247, Val Loss = 0.04748610034584999\n",
            "Epoch 152: Train Loss = 0.03303482010960579, Val Loss = 0.026278182864189148\n",
            "Epoch 153: Train Loss = 0.04935189336538315, Val Loss = 0.07710585743188858\n",
            "Epoch 154: Train Loss = 0.056302156299352646, Val Loss = 0.015564833767712116\n",
            "Epoch 155: Train Loss = 0.03270288184285164, Val Loss = 0.013591211289167404\n",
            "Epoch 156: Train Loss = 0.012286465615034103, Val Loss = 0.037095796316862106\n",
            "Epoch 157: Train Loss = 0.024990033358335495, Val Loss = 0.017085952684283257\n",
            "Epoch 158: Train Loss = 0.03526895493268967, Val Loss = 0.02822999097406864\n",
            "Epoch 159: Train Loss = 0.019092319533228874, Val Loss = 0.018444430083036423\n",
            "Epoch 160: Train Loss = 0.013832973316311836, Val Loss = 0.01227780431509018\n",
            "Epoch 161: Train Loss = 0.02670925110578537, Val Loss = 0.03134799376130104\n",
            "Epoch 162: Train Loss = 0.020959004759788513, Val Loss = 0.013633046299219131\n",
            "Epoch 163: Train Loss = 0.012189457193017006, Val Loss = 0.009929121471941471\n",
            "Epoch 164: Train Loss = 0.021349472925066948, Val Loss = 0.029024526476860046\n",
            "Epoch 165: Train Loss = 0.019442444667220116, Val Loss = 0.012563795782625675\n",
            "Epoch 166: Train Loss = 0.01200306136161089, Val Loss = 0.009021417237818241\n",
            "Epoch 167: Train Loss = 0.018617168068885803, Val Loss = 0.02565089985728264\n",
            "Epoch 168: Train Loss = 0.017305471003055573, Val Loss = 0.012852257117629051\n",
            "Epoch 169: Train Loss = 0.01195914763957262, Val Loss = 0.00858278851956129\n",
            "Epoch 170: Train Loss = 0.01707395538687706, Val Loss = 0.02222118154168129\n",
            "Epoch 171: Train Loss = 0.015390650369226933, Val Loss = 0.013607658445835114\n",
            "Epoch 172: Train Loss = 0.01199764758348465, Val Loss = 0.008502108044922352\n",
            "Epoch 173: Train Loss = 0.015969328582286835, Val Loss = 0.01934576779603958\n",
            "Epoch 174: Train Loss = 0.013891798444092274, Val Loss = 0.014440280385315418\n",
            "Epoch 175: Train Loss = 0.01210670918226242, Val Loss = 0.008418730460107327\n",
            "Epoch 176: Train Loss = 0.015009845606982708, Val Loss = 0.016910819336771965\n",
            "Epoch 177: Train Loss = 0.012823173776268959, Val Loss = 0.015170959755778313\n",
            "Epoch 178: Train Loss = 0.012215201742947102, Val Loss = 0.008552768267691135\n",
            "Epoch 179: Train Loss = 0.014125294983386993, Val Loss = 0.015032252296805382\n",
            "Epoch 180: Train Loss = 0.01213123183697462, Val Loss = 0.015508527867496014\n",
            "Epoch 181: Train Loss = 0.012252425774931908, Val Loss = 0.008704148232936859\n",
            "Epoch 182: Train Loss = 0.013339203782379627, Val Loss = 0.013659154064953327\n",
            "Epoch 183: Train Loss = 0.01172086875885725, Val Loss = 0.015583490952849388\n",
            "Epoch 184: Train Loss = 0.012192022055387497, Val Loss = 0.00896396767348051\n",
            "Epoch 185: Train Loss = 0.012687216512858868, Val Loss = 0.012671355158090591\n",
            "Epoch 186: Train Loss = 0.011485666036605835, Val Loss = 0.015358379110693932\n",
            "Epoch 187: Train Loss = 0.012046595104038715, Val Loss = 0.009282248094677925\n",
            "Epoch 188: Train Loss = 0.012177569791674614, Val Loss = 0.012063708156347275\n",
            "Epoch 189: Train Loss = 0.011336677707731724, Val Loss = 0.014976553618907928\n",
            "Epoch 190: Train Loss = 0.011846578679978848, Val Loss = 0.009544421918690205\n",
            "Epoch 191: Train Loss = 0.011790703982114792, Val Loss = 0.011651420034468174\n",
            "Epoch 192: Train Loss = 0.01121591217815876, Val Loss = 0.014527098275721073\n",
            "Epoch 193: Train Loss = 0.011622569523751736, Val Loss = 0.009814657270908356\n",
            "Epoch 194: Train Loss = 0.011495211161673069, Val Loss = 0.01144326850771904\n",
            "Epoch 195: Train Loss = 0.011095356196165085, Val Loss = 0.014035077765583992\n",
            "Epoch 196: Train Loss = 0.011394733563065529, Val Loss = 0.009980971924960613\n",
            "Epoch 197: Train Loss = 0.01125982217490673, Val Loss = 0.0113532654941082\n",
            "Epoch 198: Train Loss = 0.010965460911393166, Val Loss = 0.013591757975518703\n",
            "Epoch 199: Train Loss = 0.011172955855727196, Val Loss = 0.01010661106556654\n",
            "Epoch 200: Train Loss = 0.011059927754104137, Val Loss = 0.01133753452450037\n",
            "Epoch 201: Train Loss = 0.010825825855135918, Val Loss = 0.013153407722711563\n",
            "Epoch 202: Train Loss = 0.010959053412079811, Val Loss = 0.010180383920669556\n",
            "Epoch 203: Train Loss = 0.010877645574510098, Val Loss = 0.011402781121432781\n",
            "Epoch 204: Train Loss = 0.010679087601602077, Val Loss = 0.012753925286233425\n",
            "Epoch 205: Train Loss = 0.010751600377261639, Val Loss = 0.010213417932391167\n",
            "Epoch 206: Train Loss = 0.01070153433829546, Val Loss = 0.011493926867842674\n",
            "Epoch 207: Train Loss = 0.0105282012373209, Val Loss = 0.012379808351397514\n",
            "Epoch 208: Train Loss = 0.010549110360443592, Val Loss = 0.010256323032081127\n",
            "Epoch 209: Train Loss = 0.01052405871450901, Val Loss = 0.011609053239226341\n",
            "Epoch 210: Train Loss = 0.010374887846410275, Val Loss = 0.011999490670859814\n",
            "Epoch 211: Train Loss = 0.010351122356951237, Val Loss = 0.010292349383234978\n",
            "Epoch 212: Train Loss = 0.010340869426727295, Val Loss = 0.011723815463483334\n",
            "Epoch 213: Train Loss = 0.010219461284577847, Val Loss = 0.011645307764410973\n",
            "Epoch 214: Train Loss = 0.010159393772482872, Val Loss = 0.010366080328822136\n",
            "Epoch 215: Train Loss = 0.010150426998734474, Val Loss = 0.011795383878052235\n",
            "Epoch 216: Train Loss = 0.010059980675578117, Val Loss = 0.011291135102510452\n",
            "Epoch 217: Train Loss = 0.009975999593734741, Val Loss = 0.01048280019313097\n",
            "Epoch 218: Train Loss = 0.009954322129487991, Val Loss = 0.011808997020125389\n",
            "Epoch 219: Train Loss = 0.009892922826111317, Val Loss = 0.010970307514071465\n",
            "Epoch 220: Train Loss = 0.009801864624023438, Val Loss = 0.010642066597938538\n",
            "Epoch 221: Train Loss = 0.009758178144693375, Val Loss = 0.01172514259815216\n",
            "Epoch 222: Train Loss = 0.009715692140161991, Val Loss = 0.010713816620409489\n",
            "Epoch 223: Train Loss = 0.009634474292397499, Val Loss = 0.010847474448382854\n",
            "Epoch 224: Train Loss = 0.009569713845849037, Val Loss = 0.011536168865859509\n",
            "Epoch 225: Train Loss = 0.009529809467494488, Val Loss = 0.010545050725340843\n",
            "Epoch 226: Train Loss = 0.009467163123190403, Val Loss = 0.011062664911150932\n",
            "Epoch 227: Train Loss = 0.009393406100571156, Val Loss = 0.011272204108536243\n",
            "Epoch 228: Train Loss = 0.00934239849448204, Val Loss = 0.010505820624530315\n",
            "Epoch 229: Train Loss = 0.009293991141021252, Val Loss = 0.011228467337787151\n",
            "Epoch 230: Train Loss = 0.009227374568581581, Val Loss = 0.01097243744879961\n",
            "Epoch 231: Train Loss = 0.009164400398731232, Val Loss = 0.01059827022254467\n",
            "Epoch 232: Train Loss = 0.009116369299590588, Val Loss = 0.011281394399702549\n",
            "Epoch 233: Train Loss = 0.009063382633030415, Val Loss = 0.010728329420089722\n",
            "Epoch 234: Train Loss = 0.009000768885016441, Val Loss = 0.01077835913747549\n",
            "Epoch 235: Train Loss = 0.008945508860051632, Val Loss = 0.011201434768736362\n",
            "Epoch 236: Train Loss = 0.008898421190679073, Val Loss = 0.010590593330562115\n",
            "Epoch 237: Train Loss = 0.00884545873850584, Val Loss = 0.01099526695907116\n",
            "Epoch 238: Train Loss = 0.008787840604782104, Val Loss = 0.010976760648190975\n",
            "Epoch 239: Train Loss = 0.008736747317016125, Val Loss = 0.010607090778648853\n",
            "Epoch 240: Train Loss = 0.008690650574862957, Val Loss = 0.011111385188996792\n",
            "Epoch 241: Train Loss = 0.008640718646347523, Val Loss = 0.010801105760037899\n",
            "Epoch 242: Train Loss = 0.008587044663727283, Val Loss = 0.010763599537312984\n",
            "Epoch 243: Train Loss = 0.008539089933037758, Val Loss = 0.01106877252459526\n",
            "Epoch 244: Train Loss = 0.00849447026848793, Val Loss = 0.010641304776072502\n",
            "Epoch 245: Train Loss = 0.008447704836726189, Val Loss = 0.010945332236588001\n",
            "Epoch 246: Train Loss = 0.008400045335292816, Val Loss = 0.010938485153019428\n",
            "Epoch 247: Train Loss = 0.00835578702390194, Val Loss = 0.010691327042877674\n",
            "Epoch 248: Train Loss = 0.008314104750752449, Val Loss = 0.01101282425224781\n",
            "Epoch 249: Train Loss = 0.00827211607247591, Val Loss = 0.010715730488300323\n",
            "Epoch 250: Train Loss = 0.008229327388107777, Val Loss = 0.01082608662545681\n",
            "Epoch 251: Train Loss = 0.008188165724277496, Val Loss = 0.010953751392662525\n",
            "Epoch 252: Train Loss = 0.00814991258084774, Val Loss = 0.010678926482796669\n",
            "Epoch 253: Train Loss = 0.008112585172057152, Val Loss = 0.01094042044132948\n",
            "Epoch 254: Train Loss = 0.008074846118688583, Val Loss = 0.01075818296521902\n",
            "Epoch 255: Train Loss = 0.008037690073251724, Val Loss = 0.010767784900963306\n",
            "Epoch 256: Train Loss = 0.008002469316124916, Val Loss = 0.010909848846495152\n",
            "Epoch 257: Train Loss = 0.007968833670020103, Val Loss = 0.010671478696167469\n",
            "Epoch 258: Train Loss = 0.007935863919556141, Val Loss = 0.010883180424571037\n",
            "Epoch 259: Train Loss = 0.007903004996478558, Val Loss = 0.010752533562481403\n",
            "Epoch 260: Train Loss = 0.007870848290622234, Val Loss = 0.01073875930160284\n",
            "Epoch 261: Train Loss = 0.007840016856789589, Val Loss = 0.010857128538191319\n",
            "Epoch 262: Train Loss = 0.00781030161306262, Val Loss = 0.010656079277396202\n",
            "Epoch 263: Train Loss = 0.007781046908348799, Val Loss = 0.010831326246261597\n",
            "Epoch 264: Train Loss = 0.007752036675810814, Val Loss = 0.010701674036681652\n",
            "Test Loss: 0.08319875597953796\n",
            "Instrument Rankings:\n",
            "Rank 1: Instrument yield_spread_LRC30APR_Index_GT5_Govt (2.7752881050109863)\n",
            "Rank 2: Instrument yield_spread_LRC30APR_Index_USGG5YR_Index (2.7615950107574463)\n",
            "Rank 3: Instrument yield_spread_LRC30APR_Index_MTGEFNCL_Index (1.4491406679153442)\n",
            "Rank 4: Instrument yield_spread_MTGEFNCL_Index_USGG5YR_Index (1.3362627029418945)\n",
            "Rank 5: Instrument yield_spread_MTGEFNCL_Index_GT5_Govt (1.327183485031128)\n",
            "Rank 6: Instrument yield_spread_GT5_Govt_USGG5YR Index (0.024341348558664322)\n",
            "Rank 7: Instrument yield_spread_USGG5YR_Index_GT5_Govt (-0.016540464013814926)\n",
            "Rank 8: Instrument yield_spread_GT5_Govt_MTGEFNCL_Index (-1.3120348453521729)\n",
            "Rank 9: Instrument yield_spread_USGG5YR_Index_MTGEFNCL_Index (-1.3356200456619263)\n",
            "Rank 10: Instrument yield_spread_MTGEFNCL_Index_LRC30APR_Index (-1.4662188291549683)\n",
            "Rank 11: Instrument yield_spread_GT5_Govt_LRC30APR_Index (-2.793123245239258)\n",
            "Rank 12: Instrument yield_spread_USGG5YR_Index_LRC30APR_Index (-2.8014280796051025)\n"
          ]
        }
      ],
      "source": [
        "# initialization code\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# path to drive where csv file is read from\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/MyDrive/BBGdatasets/sampleDailyInputData_12-27-21.csv\"\n",
        "\n",
        "# load historical data from path into a pandas DataFrame\n",
        "df = pd.read_csv(datadir, index_col=0, header=[0,1], parse_dates=True)\n",
        "df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "\n",
        "# transform existing DataFrame by calculating yield spreads and appending to end\n",
        "df['yield_spread_MTGEFNCL_Index_LRC30APR_Index'] = df['MTGEFNCL Index PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_MTGEFNCL_Index_GT5_Govt'] = df['MTGEFNCL Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "df['yield_spread_MTGEFNCL_Index_USGG5YR_Index'] = df['MTGEFNCL Index PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_MTGEFNCL_Index'] = df['LRC30APR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_GT5_Govt'] = df['LRC30APR Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_USGG5YR_Index'] = df['LRC30APR Index PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_MTGEFNCL_Index'] = df['GT5 Govt PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_LRC30APR_Index'] = df['GT5 Govt PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_USGG5YR Index'] = df['GT5 Govt PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_MTGEFNCL_Index'] = df['USGG5YR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_LRC30APR_Index'] = df['USGG5YR Index PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_GT5_Govt'] = df['USGG5YR Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "\n",
        "# number of yield spreads, adjust accordingly\n",
        "n = 12\n",
        "\n",
        "# take yield spreads and other market information\n",
        "yield_spreads = df.iloc[:, -n:]\n",
        "other_columns = df.iloc[:, :-n]\n",
        "\n",
        "# concatenate yield spreads with other market information to restructure into new DataFrame\n",
        "df_new = pd.concat([yield_spreads, other_columns], axis=1)\n",
        "\n",
        "# assign the new dataframe to df to not overwrite original DataFrame as well as drop empty columns\n",
        "data = df_new.dropna(axis=1)\n",
        "\n",
        "# separate the yield spreads which are our targets from other market information again\n",
        "yield_spreads = data.iloc[:, :n].values\n",
        "market_info = data.iloc[:, n:].values\n",
        "\n",
        "# device configuration for using Nvidia GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# convert the data to PyTorch tensors of type torch.float32 and run on GPU if available\n",
        "X_yield_spreads = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "X_market_info = torch.tensor(market_info, dtype=torch.float32).to(device)\n",
        "\n",
        "# concatenate the yield spreads and other market information tensors as well as run on GPU if available\n",
        "X = torch.cat((X_yield_spreads, X_market_info), dim=1).to(device)\n",
        "y = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "\n",
        "# split the data into training, validation, and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n",
        "\n",
        "# note that no scalers are needed as features are similar\n",
        "\n",
        "# define the neural network model named Spread\n",
        "class Spread(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output):\n",
        "        super(Spread, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.fc3 = nn.Linear(hidden_size_2, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# hyperparameters to control learning proccess of model\n",
        "input_size = data.shape[1]\n",
        "hidden_size_1 = 264\n",
        "hidden_size_2 = 132\n",
        "output = n\n",
        "num_epochs = input_size * 11\n",
        "learning_rate = 0.01\n",
        "\n",
        "# instantiating the neural network model with GPU if available\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "\n",
        "# define the Mean Squred Error loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop of model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # convert x_train and y_train to run on GPU if available\n",
        "    x_train = x_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    # update learnable weights of model as well as output input of x_train set\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    # MSE loss between x_train outputs and actual y_train values\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # evaluate model on validation set without calculating gradients\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_val = x_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "    # display current Train Loss and Val Loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {loss.item()}, Val Loss = {val_loss.item()}\")\n",
        "\n",
        "save_model_name = 'spread.pt'\n",
        "PATH = f\"/content/drive/MyDrive/BBGdatasets/MLmodels/{save_model_name}\"\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "# evaluate model on test set similar to validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    test_outputs = model(x_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    print(f\"Test Loss: {test_loss.item()}\")\n",
        "\n",
        "# convert the predicted tensor back to a NumPy array\n",
        "predictions = test_outputs.cpu().numpy()\n",
        "\n",
        "# calculate the average predicted yield spread for each instrument\n",
        "avg_predicted_spreads = np.mean(predictions, axis=0)\n",
        "\n",
        "# get instrument names\n",
        "instrument_names = data.columns[:n]\n",
        "\n",
        "# create a list of tuples with instrument names and their average predicted spreads\n",
        "instrument_spreads = list(zip(instrument_names, avg_predicted_spreads))\n",
        "\n",
        "# rank all of the instruments based on their average predicted yield spreads\n",
        "ranked_instruments = sorted(instrument_spreads, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# print instrument rankings\n",
        "print(\"Instrument Rankings:\")\n",
        "for rank, (instrument_name, spread) in enumerate(ranked_instruments):\n",
        "    print(f\"Rank {rank+1}: Instrument {instrument_name} ({spread})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkyJ01ifrNBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}