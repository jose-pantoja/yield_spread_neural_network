{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y0KPpHD4C7s6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc5d02b-fe1d-4e83-fa53-4344b59c2d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1: Train Loss = 34.31129837036133, Val Loss = 851.558837890625\n",
            "Epoch 2: Train Loss = 906.3295288085938, Val Loss = 56.0106315612793\n",
            "Epoch 3: Train Loss = 59.5449104309082, Val Loss = 8.62111759185791\n",
            "Epoch 4: Train Loss = 8.804179191589355, Val Loss = 4.456129550933838\n",
            "Epoch 5: Train Loss = 4.498026371002197, Val Loss = 1.6297476291656494\n",
            "Epoch 6: Train Loss = 1.642369270324707, Val Loss = 1.1768614053726196\n",
            "Epoch 7: Train Loss = 1.5130195617675781, Val Loss = 0.6636648774147034\n",
            "Epoch 8: Train Loss = 0.8689082264900208, Val Loss = 0.8619053363800049\n",
            "Epoch 9: Train Loss = 0.9122878909111023, Val Loss = 0.7465013265609741\n",
            "Epoch 10: Train Loss = 0.7525621056556702, Val Loss = 0.6420291662216187\n",
            "Epoch 11: Train Loss = 0.581461489200592, Val Loss = 0.40661144256591797\n",
            "Epoch 12: Train Loss = 0.34705692529678345, Val Loss = 0.38190940022468567\n",
            "Epoch 13: Train Loss = 0.3234364688396454, Val Loss = 0.27051663398742676\n",
            "Epoch 14: Train Loss = 0.23990902304649353, Val Loss = 0.14923341572284698\n",
            "Epoch 15: Train Loss = 0.16022758185863495, Val Loss = 0.12990130484104156\n",
            "Epoch 16: Train Loss = 0.17338977754116058, Val Loss = 0.23285561800003052\n",
            "Epoch 17: Train Loss = 0.28704890608787537, Val Loss = 0.15252886712551117\n",
            "Epoch 18: Train Loss = 0.205430269241333, Val Loss = 0.12935751676559448\n",
            "Epoch 19: Train Loss = 0.17429955303668976, Val Loss = 0.10471208393573761\n",
            "Epoch 20: Train Loss = 0.12302341312170029, Val Loss = 0.12697581946849823\n",
            "Epoch 21: Train Loss = 0.12587083876132965, Val Loss = 0.1464778333902359\n",
            "Epoch 22: Train Loss = 0.1340179741382599, Val Loss = 0.1385038048028946\n",
            "Epoch 23: Train Loss = 0.12182175368070602, Val Loss = 0.10501885414123535\n",
            "Epoch 24: Train Loss = 0.0892573818564415, Val Loss = 0.06998381018638611\n",
            "Epoch 25: Train Loss = 0.05962615832686424, Val Loss = 0.053111135959625244\n",
            "Epoch 26: Train Loss = 0.05152576044201851, Val Loss = 0.05387067794799805\n",
            "Epoch 27: Train Loss = 0.061182085424661636, Val Loss = 0.057801805436611176\n",
            "Epoch 28: Train Loss = 0.0700177326798439, Val Loss = 0.05455830693244934\n",
            "Epoch 29: Train Loss = 0.06526511907577515, Val Loss = 0.04784248769283295\n",
            "Epoch 30: Train Loss = 0.05163009092211723, Val Loss = 0.047548558562994\n",
            "Epoch 31: Train Loss = 0.042647045105695724, Val Loss = 0.05575873702764511\n",
            "Epoch 32: Train Loss = 0.04448509216308594, Val Loss = 0.06415215879678726\n",
            "Epoch 33: Train Loss = 0.0512751005589962, Val Loss = 0.06338386982679367\n",
            "Epoch 34: Train Loss = 0.0539022758603096, Val Loss = 0.052143558859825134\n",
            "Epoch 35: Train Loss = 0.04961268976330757, Val Loss = 0.03714273124933243\n",
            "Epoch 36: Train Loss = 0.042858023196458817, Val Loss = 0.026342807337641716\n",
            "Epoch 37: Train Loss = 0.03942501172423363, Val Loss = 0.022434348240494728\n",
            "Epoch 38: Train Loss = 0.04050609841942787, Val Loss = 0.021844616159796715\n",
            "Epoch 39: Train Loss = 0.04180910438299179, Val Loss = 0.020212635397911072\n",
            "Epoch 40: Train Loss = 0.03890909254550934, Val Loss = 0.017818082123994827\n",
            "Epoch 41: Train Loss = 0.03249598667025566, Val Loss = 0.01847609505057335\n",
            "Epoch 42: Train Loss = 0.027108389884233475, Val Loss = 0.024166131392121315\n",
            "Epoch 43: Train Loss = 0.02575642615556717, Val Loss = 0.03236624225974083\n",
            "Epoch 44: Train Loss = 0.027358977124094963, Val Loss = 0.03825073316693306\n",
            "Epoch 45: Train Loss = 0.0287543423473835, Val Loss = 0.03881670907139778\n",
            "Epoch 46: Train Loss = 0.02810460701584816, Val Loss = 0.035244300961494446\n",
            "Epoch 47: Train Loss = 0.026462344452738762, Val Loss = 0.031199000775814056\n",
            "Epoch 48: Train Loss = 0.02598569728434086, Val Loss = 0.029069915413856506\n",
            "Epoch 49: Train Loss = 0.027005324140191078, Val Loss = 0.02853812463581562\n",
            "Epoch 50: Train Loss = 0.02789202518761158, Val Loss = 0.02814473956823349\n",
            "Epoch 51: Train Loss = 0.027305331081151962, Val Loss = 0.027210477739572525\n",
            "Epoch 52: Train Loss = 0.025565270334482193, Val Loss = 0.026433711871504784\n",
            "Epoch 53: Train Loss = 0.024165228009223938, Val Loss = 0.02667398191988468\n",
            "Epoch 54: Train Loss = 0.02402137592434883, Val Loss = 0.027460074052214622\n",
            "Epoch 55: Train Loss = 0.02444673329591751, Val Loss = 0.02741224132478237\n",
            "Epoch 56: Train Loss = 0.024330059066414833, Val Loss = 0.025651777163147926\n",
            "Epoch 57: Train Loss = 0.023493468761444092, Val Loss = 0.02260657213628292\n",
            "Epoch 58: Train Loss = 0.022608790546655655, Val Loss = 0.019764892756938934\n",
            "Epoch 59: Train Loss = 0.022348608821630478, Val Loss = 0.018372755497694016\n",
            "Epoch 60: Train Loss = 0.02255064621567726, Val Loss = 0.01853206567466259\n",
            "Epoch 61: Train Loss = 0.022491583600640297, Val Loss = 0.019592035561800003\n",
            "Epoch 62: Train Loss = 0.02194269746541977, Val Loss = 0.020869169384241104\n",
            "Epoch 63: Train Loss = 0.021319923922419548, Val Loss = 0.02209419570863247\n",
            "Epoch 64: Train Loss = 0.02110304683446884, Val Loss = 0.023231372237205505\n",
            "Epoch 65: Train Loss = 0.02128957211971283, Val Loss = 0.023998292163014412\n",
            "Epoch 66: Train Loss = 0.021410049870610237, Val Loss = 0.02396506443619728\n",
            "Epoch 67: Train Loss = 0.021199841052293777, Val Loss = 0.022951150313019753\n",
            "Epoch 68: Train Loss = 0.02085370570421219, Val Loss = 0.02129822038114071\n",
            "Epoch 69: Train Loss = 0.020644167438149452, Val Loss = 0.019777050241827965\n",
            "Epoch 70: Train Loss = 0.020611673593521118, Val Loss = 0.018993569537997246\n",
            "Epoch 71: Train Loss = 0.020521415397524834, Val Loss = 0.018998192623257637\n",
            "Epoch 72: Train Loss = 0.020235903561115265, Val Loss = 0.01940528117120266\n",
            "Epoch 73: Train Loss = 0.019916990771889687, Val Loss = 0.019764041528105736\n",
            "Epoch 74: Train Loss = 0.01975688338279724, Val Loss = 0.01993289589881897\n",
            "Epoch 75: Train Loss = 0.019758770242333412, Val Loss = 0.01995210535824299\n",
            "Epoch 76: Train Loss = 0.019745899364352226, Val Loss = 0.019786737859249115\n",
            "Epoch 77: Train Loss = 0.019608112052083015, Val Loss = 0.01932680793106556\n",
            "Epoch 78: Train Loss = 0.01943889819085598, Val Loss = 0.018611015751957893\n",
            "Epoch 79: Train Loss = 0.01934756711125374, Val Loss = 0.017995541915297508\n",
            "Epoch 80: Train Loss = 0.01932508498430252, Val Loss = 0.017900710925459862\n",
            "Epoch 81: Train Loss = 0.019266603514552116, Val Loss = 0.018388409167528152\n",
            "Epoch 82: Train Loss = 0.019124817103147507, Val Loss = 0.019109081476926804\n",
            "Epoch 83: Train Loss = 0.018973754718899727, Val Loss = 0.019639212638139725\n",
            "Epoch 84: Train Loss = 0.018873194232583046, Val Loss = 0.019849222153425217\n",
            "Epoch 85: Train Loss = 0.018802283331751823, Val Loss = 0.0198375154286623\n",
            "Epoch 86: Train Loss = 0.018697839230298996, Val Loss = 0.019643552601337433\n",
            "Epoch 87: Train Loss = 0.01855405978858471, Val Loss = 0.019209999591112137\n",
            "Epoch 88: Train Loss = 0.018429668620228767, Val Loss = 0.01859026588499546\n",
            "Epoch 89: Train Loss = 0.018352670595049858, Val Loss = 0.01806817203760147\n",
            "Epoch 90: Train Loss = 0.018295245245099068, Val Loss = 0.017913471907377243\n",
            "Epoch 91: Train Loss = 0.01821454055607319, Val Loss = 0.01808982342481613\n",
            "Epoch 92: Train Loss = 0.01811528019607067, Val Loss = 0.018311789259314537\n",
            "Epoch 93: Train Loss = 0.018030043691396713, Val Loss = 0.0183724295347929\n",
            "Epoch 94: Train Loss = 0.017962250858545303, Val Loss = 0.018308600410819054\n",
            "Epoch 95: Train Loss = 0.017888035625219345, Val Loss = 0.018216243013739586\n",
            "Epoch 96: Train Loss = 0.01779055781662464, Val Loss = 0.018075110390782356\n",
            "Epoch 97: Train Loss = 0.017688078805804253, Val Loss = 0.01783968321979046\n",
            "Epoch 98: Train Loss = 0.01760093681514263, Val Loss = 0.017621150240302086\n",
            "Epoch 99: Train Loss = 0.01752546802163124, Val Loss = 0.0176139697432518\n",
            "Epoch 100: Train Loss = 0.01744501106441021, Val Loss = 0.01784302107989788\n",
            "Epoch 101: Train Loss = 0.017355991527438164, Val Loss = 0.018114501610398293\n",
            "Epoch 102: Train Loss = 0.017272399738430977, Val Loss = 0.018233628943562508\n",
            "Epoch 103: Train Loss = 0.017198074609041214, Val Loss = 0.018193373456597328\n",
            "Epoch 104: Train Loss = 0.017124049365520477, Val Loss = 0.01807503215968609\n",
            "Epoch 105: Train Loss = 0.017041634768247604, Val Loss = 0.01788788102567196\n",
            "Epoch 106: Train Loss = 0.01695648580789566, Val Loss = 0.017602503299713135\n",
            "Epoch 107: Train Loss = 0.016877612099051476, Val Loss = 0.017303049564361572\n",
            "Epoch 108: Train Loss = 0.01680406928062439, Val Loss = 0.017153751105070114\n",
            "Epoch 109: Train Loss = 0.016728578135371208, Val Loss = 0.017195159569382668\n",
            "Epoch 110: Train Loss = 0.016648728400468826, Val Loss = 0.01729772426187992\n",
            "Epoch 111: Train Loss = 0.01657009683549404, Val Loss = 0.01733817346394062\n",
            "Epoch 112: Train Loss = 0.016494804993271828, Val Loss = 0.01732497662305832\n",
            "Epoch 113: Train Loss = 0.01641964726150036, Val Loss = 0.017302652820944786\n",
            "Epoch 114: Train Loss = 0.01634138636291027, Val Loss = 0.01724334992468357\n",
            "Epoch 115: Train Loss = 0.016263000667095184, Val Loss = 0.01711607351899147\n",
            "Epoch 116: Train Loss = 0.016187837347388268, Val Loss = 0.016987767070531845\n",
            "Epoch 117: Train Loss = 0.016115054488182068, Val Loss = 0.016955796629190445\n",
            "Epoch 118: Train Loss = 0.016041310504078865, Val Loss = 0.017007360234856606\n",
            "Epoch 119: Train Loss = 0.015966853126883507, Val Loss = 0.017038315534591675\n",
            "Epoch 120: Train Loss = 0.01589360274374485, Val Loss = 0.01700376719236374\n",
            "Epoch 121: Train Loss = 0.015821903944015503, Val Loss = 0.01693914085626602\n",
            "Epoch 122: Train Loss = 0.015749970450997353, Val Loss = 0.016853531822562218\n",
            "Epoch 123: Train Loss = 0.015677032992243767, Val Loss = 0.016720933839678764\n",
            "Epoch 124: Train Loss = 0.015604345127940178, Val Loss = 0.016561010852456093\n",
            "Epoch 125: Train Loss = 0.015533053316175938, Val Loss = 0.016452398151159286\n",
            "Epoch 126: Train Loss = 0.01546244602650404, Val Loss = 0.016431640833616257\n",
            "Epoch 127: Train Loss = 0.015391401015222073, Val Loss = 0.016448471695184708\n",
            "Epoch 128: Train Loss = 0.015320460312068462, Val Loss = 0.01644854247570038\n",
            "Epoch 129: Train Loss = 0.015250515192747116, Val Loss = 0.016434453427791595\n",
            "Epoch 130: Train Loss = 0.01518146600574255, Val Loss = 0.0164139736443758\n",
            "Epoch 131: Train Loss = 0.015112342312932014, Val Loss = 0.016359351575374603\n",
            "Epoch 132: Train Loss = 0.01504342257976532, Val Loss = 0.01626492291688919\n",
            "Epoch 133: Train Loss = 0.014975214377045631, Val Loss = 0.01617450825870037\n",
            "Epoch 134: Train Loss = 0.014907731674611568, Val Loss = 0.016128478571772575\n",
            "Epoch 135: Train Loss = 0.014840328134596348, Val Loss = 0.01610902138054371\n",
            "Epoch 136: Train Loss = 0.014772819355130196, Val Loss = 0.016080081462860107\n",
            "Epoch 137: Train Loss = 0.014705565758049488, Val Loss = 0.01604021154344082\n",
            "Epoch 138: Train Loss = 0.01463896781206131, Val Loss = 0.015996580943465233\n",
            "Epoch 139: Train Loss = 0.01457274705171585, Val Loss = 0.015930868685245514\n",
            "Epoch 140: Train Loss = 0.014506939798593521, Val Loss = 0.015831274911761284\n",
            "Epoch 141: Train Loss = 0.014441518113017082, Val Loss = 0.015733923763036728\n",
            "Epoch 142: Train Loss = 0.01437660213559866, Val Loss = 0.01567084714770317\n",
            "Epoch 143: Train Loss = 0.014312079176306725, Val Loss = 0.01562986895442009\n",
            "Epoch 144: Train Loss = 0.014247819781303406, Val Loss = 0.015591764822602272\n",
            "Epoch 145: Train Loss = 0.014183939434587955, Val Loss = 0.01556168869137764\n",
            "Epoch 146: Train Loss = 0.014120573177933693, Val Loss = 0.015537582337856293\n",
            "Epoch 147: Train Loss = 0.014057672582566738, Val Loss = 0.01549523789435625\n",
            "Epoch 148: Train Loss = 0.013995204120874405, Val Loss = 0.015430710278451443\n",
            "Epoch 149: Train Loss = 0.013933165930211544, Val Loss = 0.015365146100521088\n",
            "Epoch 150: Train Loss = 0.013871605508029461, Val Loss = 0.015313584357500076\n",
            "Epoch 151: Train Loss = 0.013810493983328342, Val Loss = 0.015267525799572468\n",
            "Epoch 152: Train Loss = 0.013749722391366959, Val Loss = 0.015222015790641308\n",
            "Epoch 153: Train Loss = 0.013689354993402958, Val Loss = 0.015182764269411564\n",
            "Epoch 154: Train Loss = 0.013629462569952011, Val Loss = 0.015142729505896568\n",
            "Epoch 155: Train Loss = 0.013570166192948818, Val Loss = 0.015083297155797482\n",
            "Epoch 156: Train Loss = 0.01351126004010439, Val Loss = 0.015011935494840145\n",
            "Epoch 157: Train Loss = 0.013452745042741299, Val Loss = 0.0149452593177557\n",
            "Epoch 158: Train Loss = 0.013394683599472046, Val Loss = 0.014884206466376781\n",
            "Epoch 159: Train Loss = 0.013337002135813236, Val Loss = 0.014827142469584942\n",
            "Epoch 160: Train Loss = 0.013279670849442482, Val Loss = 0.014782109297811985\n",
            "Epoch 161: Train Loss = 0.013222724199295044, Val Loss = 0.014751066453754902\n",
            "Epoch 162: Train Loss = 0.013166246004402637, Val Loss = 0.014714684337377548\n",
            "Epoch 163: Train Loss = 0.013110143132507801, Val Loss = 0.014667141251266003\n",
            "Epoch 164: Train Loss = 0.01305438857525587, Val Loss = 0.014618557877838612\n",
            "Epoch 165: Train Loss = 0.012998978607356548, Val Loss = 0.014565778896212578\n",
            "Epoch 166: Train Loss = 0.012943975627422333, Val Loss = 0.014508131891489029\n",
            "Epoch 167: Train Loss = 0.012889363802969456, Val Loss = 0.014454255811870098\n",
            "Epoch 168: Train Loss = 0.012835065834224224, Val Loss = 0.014408634975552559\n",
            "Epoch 169: Train Loss = 0.012781105004251003, Val Loss = 0.014366361312568188\n",
            "Epoch 170: Train Loss = 0.01272745430469513, Val Loss = 0.014318324625492096\n",
            "Epoch 171: Train Loss = 0.01267410907894373, Val Loss = 0.01426590234041214\n",
            "Epoch 172: Train Loss = 0.01262111309915781, Val Loss = 0.014214562252163887\n",
            "Epoch 173: Train Loss = 0.012568450532853603, Val Loss = 0.014161058701574802\n",
            "Epoch 174: Train Loss = 0.01251615397632122, Val Loss = 0.014099955558776855\n",
            "Epoch 175: Train Loss = 0.0124642513692379, Val Loss = 0.014050417579710484\n",
            "Epoch 176: Train Loss = 0.012412688694894314, Val Loss = 0.01401318609714508\n",
            "Epoch 177: Train Loss = 0.012361467815935612, Val Loss = 0.013965618796646595\n",
            "Epoch 178: Train Loss = 0.012310587801039219, Val Loss = 0.013919073157012463\n",
            "Epoch 179: Train Loss = 0.012260045856237411, Val Loss = 0.013882767409086227\n",
            "Epoch 180: Train Loss = 0.012209861539304256, Val Loss = 0.013831784017384052\n",
            "Epoch 181: Train Loss = 0.012160053476691246, Val Loss = 0.013774402439594269\n",
            "Epoch 182: Train Loss = 0.012110658921301365, Val Loss = 0.013730901293456554\n",
            "Epoch 183: Train Loss = 0.012061630375683308, Val Loss = 0.01368643343448639\n",
            "Epoch 184: Train Loss = 0.012012984603643417, Val Loss = 0.013632618822157383\n",
            "Epoch 185: Train Loss = 0.011964702047407627, Val Loss = 0.013590673916041851\n",
            "Epoch 186: Train Loss = 0.011916828341782093, Val Loss = 0.013550982810556889\n",
            "Epoch 187: Train Loss = 0.01186930201947689, Val Loss = 0.01349719613790512\n",
            "Epoch 188: Train Loss = 0.0118221091106534, Val Loss = 0.013445083983242512\n",
            "Epoch 189: Train Loss = 0.01177524495869875, Val Loss = 0.013403452932834625\n",
            "Epoch 190: Train Loss = 0.01172879058867693, Val Loss = 0.013353009708225727\n",
            "Epoch 191: Train Loss = 0.011682769283652306, Val Loss = 0.013300453312695026\n",
            "Epoch 192: Train Loss = 0.011637082323431969, Val Loss = 0.013263264670968056\n",
            "Epoch 193: Train Loss = 0.011591683141887188, Val Loss = 0.013221265748143196\n",
            "Epoch 194: Train Loss = 0.011546570807695389, Val Loss = 0.013170233927667141\n",
            "Epoch 195: Train Loss = 0.011501802131533623, Val Loss = 0.013131712563335896\n",
            "Epoch 196: Train Loss = 0.011457320302724838, Val Loss = 0.013091091997921467\n",
            "Epoch 197: Train Loss = 0.011413161642849445, Val Loss = 0.013037916272878647\n",
            "Epoch 198: Train Loss = 0.011369340121746063, Val Loss = 0.012995371595025063\n",
            "Epoch 199: Train Loss = 0.011325828731060028, Val Loss = 0.012955473735928535\n",
            "Epoch 200: Train Loss = 0.011282646097242832, Val Loss = 0.012904448434710503\n",
            "Epoch 201: Train Loss = 0.011239796876907349, Val Loss = 0.01286296360194683\n",
            "Epoch 202: Train Loss = 0.011197264306247234, Val Loss = 0.012823034077882767\n",
            "Epoch 203: Train Loss = 0.011155037209391594, Val Loss = 0.012777592055499554\n",
            "Epoch 204: Train Loss = 0.011113109067082405, Val Loss = 0.01273118332028389\n",
            "Epoch 205: Train Loss = 0.011071491986513138, Val Loss = 0.012690882198512554\n",
            "Epoch 206: Train Loss = 0.011030128225684166, Val Loss = 0.012645590119063854\n",
            "Epoch 207: Train Loss = 0.010989060625433922, Val Loss = 0.012599867768585682\n",
            "Epoch 208: Train Loss = 0.010948367416858673, Val Loss = 0.012563586235046387\n",
            "Epoch 209: Train Loss = 0.010907970368862152, Val Loss = 0.012522404082119465\n",
            "Epoch 210: Train Loss = 0.010867849923670292, Val Loss = 0.012477531097829342\n",
            "Epoch 211: Train Loss = 0.010828080587089062, Val Loss = 0.01244073174893856\n",
            "Epoch 212: Train Loss = 0.010788590647280216, Val Loss = 0.012397355400025845\n",
            "Epoch 213: Train Loss = 0.010749383829534054, Val Loss = 0.012350278906524181\n",
            "Epoch 214: Train Loss = 0.010710475035011768, Val Loss = 0.01231431681662798\n",
            "Epoch 215: Train Loss = 0.010671881958842278, Val Loss = 0.012270616367459297\n",
            "Epoch 216: Train Loss = 0.010633610188961029, Val Loss = 0.012229888699948788\n",
            "Epoch 217: Train Loss = 0.010595591738820076, Val Loss = 0.012191965244710445\n",
            "Epoch 218: Train Loss = 0.010557878762483597, Val Loss = 0.012150019407272339\n",
            "Epoch 219: Train Loss = 0.010520471259951591, Val Loss = 0.012107602320611477\n",
            "Epoch 220: Train Loss = 0.010483330115675926, Val Loss = 0.012068036943674088\n",
            "Epoch 221: Train Loss = 0.010446514934301376, Val Loss = 0.012028385885059834\n",
            "Epoch 222: Train Loss = 0.010409964248538017, Val Loss = 0.011987718753516674\n",
            "Epoch 223: Train Loss = 0.010373679921030998, Val Loss = 0.011949457228183746\n",
            "Epoch 224: Train Loss = 0.01033772062510252, Val Loss = 0.011911410838365555\n",
            "Epoch 225: Train Loss = 0.010302080772817135, Val Loss = 0.01186988316476345\n",
            "Epoch 226: Train Loss = 0.010266701690852642, Val Loss = 0.011830473318696022\n",
            "Epoch 227: Train Loss = 0.010231660678982735, Val Loss = 0.011793524958193302\n",
            "Epoch 228: Train Loss = 0.010196908377110958, Val Loss = 0.011752664111554623\n",
            "Epoch 229: Train Loss = 0.01016243640333414, Val Loss = 0.011714580468833447\n",
            "Epoch 230: Train Loss = 0.010128337889909744, Val Loss = 0.011678909882903099\n",
            "Epoch 231: Train Loss = 0.010094523429870605, Val Loss = 0.011637531220912933\n",
            "Epoch 232: Train Loss = 0.010061045177280903, Val Loss = 0.011599971912801266\n",
            "Epoch 233: Train Loss = 0.010027929209172726, Val Loss = 0.011565483175218105\n",
            "Epoch 234: Train Loss = 0.00999508611857891, Val Loss = 0.011525183916091919\n",
            "Epoch 235: Train Loss = 0.00996251031756401, Val Loss = 0.01148949097841978\n",
            "Epoch 236: Train Loss = 0.00993020087480545, Val Loss = 0.011455166153609753\n",
            "Epoch 237: Train Loss = 0.009898141957819462, Val Loss = 0.011414333246648312\n",
            "Epoch 238: Train Loss = 0.009866350330412388, Val Loss = 0.011382008902728558\n",
            "Epoch 239: Train Loss = 0.009834899567067623, Val Loss = 0.011344583705067635\n",
            "Epoch 240: Train Loss = 0.0098037114366889, Val Loss = 0.011309542693197727\n",
            "Epoch 241: Train Loss = 0.009772783145308495, Val Loss = 0.011274833232164383\n",
            "Epoch 242: Train Loss = 0.00974209699779749, Val Loss = 0.011238393373787403\n",
            "Epoch 243: Train Loss = 0.009711680933833122, Val Loss = 0.01120508648455143\n",
            "Epoch 244: Train Loss = 0.009681536816060543, Val Loss = 0.011170419864356518\n",
            "Epoch 245: Train Loss = 0.009651674889028072, Val Loss = 0.011135898530483246\n",
            "Epoch 246: Train Loss = 0.00962206069380045, Val Loss = 0.011104459874331951\n",
            "Epoch 247: Train Loss = 0.009592699818313122, Val Loss = 0.011068648658692837\n",
            "Epoch 248: Train Loss = 0.009563579224050045, Val Loss = 0.011037828400731087\n",
            "Epoch 249: Train Loss = 0.009534736163914204, Val Loss = 0.011005491018295288\n",
            "Epoch 250: Train Loss = 0.00950619112700224, Val Loss = 0.01097133383154869\n",
            "Epoch 251: Train Loss = 0.009477928280830383, Val Loss = 0.010942239314317703\n",
            "Epoch 252: Train Loss = 0.009449916891753674, Val Loss = 0.01090885791927576\n",
            "Epoch 253: Train Loss = 0.009422125294804573, Val Loss = 0.010876794345676899\n",
            "Epoch 254: Train Loss = 0.009394566528499126, Val Loss = 0.010846687480807304\n",
            "Epoch 255: Train Loss = 0.009367245249450207, Val Loss = 0.010815231129527092\n",
            "Epoch 256: Train Loss = 0.009340163320302963, Val Loss = 0.010781618766486645\n",
            "Epoch 257: Train Loss = 0.009313283488154411, Val Loss = 0.010753078386187553\n",
            "Epoch 258: Train Loss = 0.009286628104746342, Val Loss = 0.010720633901655674\n",
            "Epoch 259: Train Loss = 0.009260193444788456, Val Loss = 0.010690159164369106\n",
            "Epoch 260: Train Loss = 0.009233961813151836, Val Loss = 0.010662976652383804\n",
            "Epoch 261: Train Loss = 0.009207943454384804, Val Loss = 0.01062935870140791\n",
            "Epoch 262: Train Loss = 0.009182129986584187, Val Loss = 0.010602723807096481\n",
            "Epoch 263: Train Loss = 0.009156526997685432, Val Loss = 0.010571692138910294\n",
            "Epoch 264: Train Loss = 0.009131127968430519, Val Loss = 0.010540979914367199\n",
            "Test Loss: 0.08157674968242645\n",
            "Instrument Rankings:\n",
            "Rank 1: Instrument yield_spread_LRC30APR_Index_USGG5YR_Index (2.8295793533325195)\n",
            "Rank 2: Instrument yield_spread_LRC30APR_Index_GT5_Govt (2.7916276454925537)\n",
            "Rank 3: Instrument yield_spread_LRC30APR_Index_MTGEFNCL_Index (1.464983344078064)\n",
            "Rank 4: Instrument yield_spread_MTGEFNCL_Index_USGG5YR_Index (1.3020215034484863)\n",
            "Rank 5: Instrument yield_spread_MTGEFNCL_Index_GT5_Govt (1.2888453006744385)\n",
            "Rank 6: Instrument yield_spread_USGG5YR_Index_GT5_Govt (0.006097337696701288)\n",
            "Rank 7: Instrument yield_spread_GT5_Govt_USGG5YR Index (0.0009301597019657493)\n",
            "Rank 8: Instrument yield_spread_USGG5YR_Index_MTGEFNCL_Index (-1.3206087350845337)\n",
            "Rank 9: Instrument yield_spread_GT5_Govt_MTGEFNCL_Index (-1.3461395502090454)\n",
            "Rank 10: Instrument yield_spread_MTGEFNCL_Index_LRC30APR_Index (-1.4825999736785889)\n",
            "Rank 11: Instrument yield_spread_USGG5YR_Index_LRC30APR_Index (-2.74178147315979)\n",
            "Rank 12: Instrument yield_spread_GT5_Govt_LRC30APR_Index (-2.7787437438964844)\n"
          ]
        }
      ],
      "source": [
        "# initialization code\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# path to drive where csv file is read from\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/MyDrive/BBGdatasets/sampleDailyInputData_12-27-21.csv\"\n",
        "\n",
        "# load historical data from path into a pandas DataFrame\n",
        "df = pd.read_csv(datadir, index_col=0, header=[0,1], parse_dates=True)\n",
        "df.columns = [' '.join(col).strip() for col in df.columns.values]\n",
        "\n",
        "# transform existing DataFrame by calculating yield spreads and appending to end\n",
        "df['yield_spread_MTGEFNCL_Index_LRC30APR_Index'] = df['MTGEFNCL Index PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_MTGEFNCL_Index_GT5_Govt'] = df['MTGEFNCL Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "df['yield_spread_MTGEFNCL_Index_USGG5YR_Index'] = df['MTGEFNCL Index PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_MTGEFNCL_Index'] = df['LRC30APR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_GT5_Govt'] = df['LRC30APR Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "df['yield_spread_LRC30APR_Index_USGG5YR_Index'] = df['LRC30APR Index PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_MTGEFNCL_Index'] = df['GT5 Govt PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_LRC30APR_Index'] = df['GT5 Govt PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_GT5_Govt_USGG5YR Index'] = df['GT5 Govt PX_LAST'] - df['USGG5YR Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_MTGEFNCL_Index'] = df['USGG5YR Index PX_LAST'] - df['MTGEFNCL Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_LRC30APR_Index'] = df['USGG5YR Index PX_LAST'] - df['LRC30APR Index PX_LAST']\n",
        "df['yield_spread_USGG5YR_Index_GT5_Govt'] = df['USGG5YR Index PX_LAST'] - df['GT5 Govt PX_LAST']\n",
        "\n",
        "# number of yield spreads, adjust accordingly\n",
        "n = 12\n",
        "\n",
        "# take yield spreads and other market information\n",
        "yield_spreads = df.iloc[:, -n:]\n",
        "other_columns = df.iloc[:, :-n]\n",
        "\n",
        "# concatenate yield spreads with other market information to restructure into new DataFrame\n",
        "df_new = pd.concat([yield_spreads, other_columns], axis=1)\n",
        "\n",
        "# assign the new dataframe to df to not overwrite original DataFrame as well as drop empty columns\n",
        "data = df_new.dropna(axis=1)\n",
        "\n",
        "# separate the yield spreads which are our targets from other market information again\n",
        "yield_spreads = data.iloc[:, :n].values\n",
        "market_info = data.iloc[:, n:].values\n",
        "\n",
        "# device configuration for using Nvidia GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# convert the data to PyTorch tensors of type torch.float32 and run on GPU if available\n",
        "X_yield_spreads = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "X_market_info = torch.tensor(market_info, dtype=torch.float32).to(device)\n",
        "\n",
        "# concatenate the yield spreads and other market information tensors as well as run on GPU if available\n",
        "X = torch.cat((X_yield_spreads, X_market_info), dim=1).to(device)\n",
        "y = torch.tensor(yield_spreads, dtype=torch.float32).to(device)\n",
        "\n",
        "# split the data into training, validation, and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False)\n",
        "\n",
        "# note that no scalers are needed as features are similar\n",
        "\n",
        "# define the neural network model named Spread\n",
        "class Spread(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size_1, hidden_size_2, output):\n",
        "        super(Spread, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size_1)\n",
        "        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        self.fc3 = nn.Linear(hidden_size_2, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# hyperparameters to control learning proccess of model\n",
        "input_size = data.shape[1]\n",
        "hidden_size_1 = 264\n",
        "hidden_size_2 = 132\n",
        "output = n\n",
        "num_epochs = input_size * 11\n",
        "learning_rate = 0.01\n",
        "\n",
        "# instantiating the neural network model with GPU if available\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "\n",
        "# define the Mean Squred Error loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop of model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # convert x_train and y_train to run on GPU if available\n",
        "    x_train = x_train.to(device)\n",
        "    y_train = y_train.to(device)\n",
        "\n",
        "    # update learnable weights of model as well as output input of x_train set\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    # MSE loss between x_train outputs and actual y_train values\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # evaluate model on validation set without calculating gradients\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_val = x_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "\n",
        "        val_outputs = model(x_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "    # display current Train Loss and Val Loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {loss.item()}, Val Loss = {val_loss.item()}\")\n",
        "\n",
        "# save only state_dict of model and where\n",
        "save_model_name = 'spread.pt'\n",
        "PATH = f\"/content/drive/MyDrive/BBGdatasets/MLmodels/{save_model_name}\"\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# load loaded dictionary of model\n",
        "model = Spread(input_size, hidden_size_1, hidden_size_2, output).to(device)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "# evaluate model on test set similar to validation set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "\n",
        "    test_outputs = model(x_test)\n",
        "    test_loss = criterion(test_outputs, y_test)\n",
        "    print(f\"Test Loss: {test_loss.item()}\")\n",
        "\n",
        "# convert the predicted tensor back to a NumPy array\n",
        "predictions = test_outputs.cpu().numpy()\n",
        "\n",
        "# calculate the average predicted yield spread for each instrument\n",
        "avg_predicted_spreads = np.mean(predictions, axis=0)\n",
        "\n",
        "# get instrument names\n",
        "instrument_names = data.columns[:n]\n",
        "\n",
        "# create a list of tuples with instrument names and their average predicted spreads\n",
        "instrument_spreads = list(zip(instrument_names, avg_predicted_spreads))\n",
        "\n",
        "# rank all of the instruments based on their average predicted yield spreads\n",
        "ranked_instruments = sorted(instrument_spreads, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# print instrument rankings\n",
        "print(\"Instrument Rankings:\")\n",
        "for rank, (instrument_name, spread) in enumerate(ranked_instruments):\n",
        "    print(f\"Rank {rank+1}: Instrument {instrument_name} ({spread})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkyJ01ifrNBg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}